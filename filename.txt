Distributed
computing

Wikipedia
Distributed
computing
From
Wikipedia
the
free
encyclopedia
Jump
to
navigation
Jump
to
search
Distributed
application
redirects
here
For
trustless
applications
see
Decentralized
application
Distributed
Information
Processing
redirects
here
For
the
computer
company
see
DIP
Research
Distributed
computing
is
a
field
of
computer
science
that
studies
distributed
systems
A
distributed
system
is
a
system
whose
components
are
located
on
different
networked
computers
which
then
communicate
and
coordinate
their
actions
by
passing
messages
to
one
another
The
components
interact
with
one
another
in
order
to
achieve
a
common
goal
Three
significant
characteristics
of
distributed
systems
are
concurrency
of
components
lack
of
a
global
clock
and
independent
failure
of
components
Examples
of
distributed
systems
vary
from
SOAbased
systems
to
massively
multiplayer
online
games
to
peertopeer
applications
A
computer
program
that
runs
within
a
distributed
system
is
called
a
distributed
program
and
distributed
programming
is
the
process
of
writing
such
programs
There
are
many
different
types
of
implementations
for
the
message
passing
mechanism
including
pure
HTTP
RPClike
connectors
and
message
queues
Distributed
computing
also
refers
to
the
use
of
distributed
systems
to
solve
computational
problems
In
distributed
computing
a
problem
is
divided
into
many
tasks
each
of
which
is
solved
by
one
or
more
computers
which
communicate
with
each
other
via
message
passing
Contents
1
Introduction
2
Parallel
and
distributed
computing
3
History
4
Architectures
5
Applications
6
Examples
7
Theoretical
foundations
71
Models
72
An
example
73
Complexity
measures
74
Other
problems
75
Properties
of
distributed
systems
8
See
also
9
Notes
10
References
11
Further
reading
12
External
links
Introduction
The
word
parallel
in
terms
such
as
parallel
system
distributed
programming
and
distributed
algorithm
originally
referred
to
computer
networks
where
individual
computers
were
physically
distributed
within
some
geographical
area
The
terms
are
nowadays
used
in
a
much
wider
sense
even
referring
to
autonomous
processes
that
run
on
the
same
physical
computer
and
interact
with
each
other
by
message
passing
While
there
is
no
single
definition
of
a
distributed
system
the
following
defining
properties
are
commonly
used
There
are
several
autonomous
computational
entities
computers
or
nodes
each
of
which
has
its
own
local
memory
The
entities
communicate
with
each
other
by
message
passing
A
distributed
system
may
have
a
common
goal
such
as
solving
a
large
computational
problem
the
user
then
perceives
the
collection
of
autonomous
processors
as
a
unit
Alternatively
each
computer
may
have
its
own
user
with
individual
needs
and
the
purpose
of
the
distributed
system
is
to
coordinate
the
use
of
shared
resources
or
provide
communication
services
to
the
users
Other
typical
properties
of
distributed
systems
include
the
following
The
system
has
to
tolerate
failures
in
individual
computers
The
structure
of
the
system
network
topology
network
latency
number
of
computers
is
not
known
in
advance
the
system
may
consist
of
different
kinds
of
computers
and
network
links
and
the
system
may
change
during
the
execution
of
a
distributed
program
Each
computer
has
only
a
limited
incomplete
view
of
the
system
Each
computer
may
know
only
one
part
of
the
input
Parallel
and
distributed
computing
a
b
a
distributed
system
c
a
parallel
system
Distributed
systems
are
groups
of
networked
computers
which
have
the
same
goal
for
their
workThe
terms
concurrent
computing
parallel
computing
and
distributed
computing
have
a
lot
of
overlap
and
no
clear
distinction
exists
between
them
The
same
system
may
be
characterized
both
as
parallel
and
distributed
the
processors
in
a
typical
distributed
system
run
concurrently
in
parallel
Parallel
computing
may
be
seen
as
a
particular
tightly
coupled
form
of
distributed
computing
and
distributed
computing
may
be
seen
as
a
loosely
coupled
form
of
parallel
computing
Nevertheless
it
is
possible
to
roughly
classify
concurrent
systems
as
parallel
or
distributed
using
the
following
criteria
In
parallel
computing
all
processors
may
have
access
to
a
shared
memory
to
exchange
information
between
processors
In
distributed
computing
each
processor
has
its
own
private
memory
distributed
memory
Information
is
exchanged
by
passing
messages
between
the
processors
The
figure
on
the
right
illustrates
the
difference
between
distributed
and
parallel
systems
Figure
a
is
a
schematic
view
of
a
typical
distributed
system
the
system
is
represented
as
a
network
topology
in
which
each
node
is
a
computer
and
each
line
connecting
the
nodes
is
a
communication
link
Figure
b
shows
the
same
distributed
system
in
more
detail
each
computer
has
its
own
local
memory
and
information
can
be
exchanged
only
by
passing
messages
from
one
node
to
another
by
using
the
available
communication
links
Figure
c
shows
a
parallel
system
in
which
each
processor
has
a
direct
access
to
a
shared
memory
The
situation
is
further
complicated
by
the
traditional
uses
of
the
terms
parallel
and
distributed
algorithm
that
do
not
quite
match
the
above
definitions
of
parallel
and
distributed
systems
see
below
for
more
detailed
discussion
Nevertheless
as
a
rule
of
thumb
highperformance
parallel
computation
in
a
sharedmemory
multiprocessor
uses
parallel
algorithms
while
the
coordination
of
a
largescale
distributed
system
uses
distributed
algorithms
History
The
use
of
concurrent
processes
that
communicate
by
messagepassing
has
its
roots
in
operating
system
architectures
studied
in
the
1960s
The
first
widespread
distributed
systems
were
localarea
networks
such
as
Ethernet
which
was
invented
in
the
1970s
ARPANET
the
predecessor
of
the
Internet
was
introduced
in
the
late
1960s
and
ARPANET
email
was
invented
in
the
early
1970s
Email
became
the
most
successful
application
of
ARPANET
and
it
is
probably
the
earliest
example
of
a
largescale
distributed
application
In
addition
to
ARPANET
and
its
successor
the
Internet
other
early
worldwide
computer
networks
included
Usenet
and
FidoNet
from
the
1980s
both
of
which
were
used
to
support
distributed
discussion
systems
The
study
of
distributed
computing
became
its
own
branch
of
computer
science
in
the
late
1970s
and
early
1980s
The
first
conference
in
the
field
Symposium
on
Principles
of
Distributed
Computing
PODC
dates
back
to
1982
and
its
counterpart
International
Symposium
on
Distributed
Computing
DISC
was
first
held
in
Ottawa
in
1985
as
the
International
Workshop
on
Distributed
Algorithms
on
Graphs
Architectures
Various
hardware
and
software
architectures
are
used
for
distributed
computing
At
a
lower
level
it
is
necessary
to
interconnect
multiple
CPUs
with
some
sort
of
network
regardless
of
whether
that
network
is
printed
onto
a
circuit
board
or
made
up
of
loosely
coupled
devices
and
cables
At
a
higher
level
it
is
necessary
to
interconnect
processes
running
on
those
CPUs
with
some
sort
of
communication
system
Distributed
programming
typically
falls
into
one
of
several
basic
architectures
client–server
threetier
ntier
or
peertopeer
or
categories
loose
coupling
or
tight
coupling
Client–server
architectures
where
smart
clients
contact
the
server
for
data
then
format
and
display
it
to
the
users
Input
at
the
client
is
committed
back
to
the
server
when
it
represents
a
permanent
change
Threetier
architectures
that
move
the
client
intelligence
to
a
middle
tier
so
that
stateless
clients
can
be
used
This
simplifies
application
deployment
Most
web
applications
are
threetier
ntier
architectures
that
refer
typically
to
web
applications
which
further
forward
their
requests
to
other
enterprise
services
This
type
of
application
is
the
one
most
responsible
for
the
success
of
application
servers
Peertopeer
architectures
where
there
are
no
special
machines
that
provide
a
service
or
manage
the
network
resources227
Instead
all
responsibilities
are
uniformly
divided
among
all
machines
known
as
peers
Peers
can
serve
both
as
clients
and
as
servers
Another
basic
aspect
of
distributed
computing
architecture
is
the
method
of
communicating
and
coordinating
work
among
concurrent
processes
Through
various
message
passing
protocols
processes
may
communicate
directly
with
one
another
typically
in
a
master/slave
relationship
Alternatively
a
databasecentric
architecture
can
enable
distributed
computing
to
be
done
without
any
form
of
direct
interprocess
communication
by
utilizing
a
shared
database
Applications
Reasons
for
using
distributed
systems
and
distributed
computing
may
include
The
very
nature
of
an
application
may
require
the
use
of
a
communication
network
that
connects
several
computers
for
example
data
produced
in
one
physical
location
and
required
in
another
location
There
are
many
cases
in
which
the
use
of
a
single
computer
would
be
possible
in
principle
but
the
use
of
a
distributed
system
is
beneficial
for
practical
reasons
For
example
it
may
be
more
costefficient
to
obtain
the
desired
level
of
performance
by
using
a
cluster
of
several
lowend
computers
in
comparison
with
a
single
highend
computer
A
distributed
system
can
provide
more
reliability
than
a
nondistributed
system
as
there
is
no
single
point
of
failure
Moreover
a
distributed
system
may
be
easier
to
expand
and
manage
than
a
monolithic
uniprocessor
system
Examples
Examples
of
distributed
systems
and
applications
of
distributed
computing
include
the
following
telecommunication
networks
telephone
networks
and
cellular
networks
computer
networks
such
as
the
Internet
wireless
sensor
networks
routing
algorithms
network
applications
World
Wide
Web
and
peertopeer
networks
massively
multiplayer
online
games
and
virtual
reality
communities
distributed
databases
and
distributed
database
management
systems
network
file
systems
distributed
information
processing
systems
such
as
banking
systems
and
airline
reservation
systems
realtime
process
control
aircraft
control
systems
industrial
control
systems
parallel
computation
scientific
computing
including
cluster
computing
and
grid
computing
and
various
volunteer
computing
projects
see
the
list
of
distributed
computing
projects
distributed
rendering
in
computer
graphics
Theoretical
foundations
Main
article
Distributed
algorithm
Models
Many
tasks
that
we
would
like
to
automate
by
using
a
computer
are
of
question–answer
type
we
would
like
to
ask
a
question
and
the
computer
should
produce
an
answer
In
theoretical
computer
science
such
tasks
are
called
computational
problems
Formally
a
computational
problem
consists
of
instances
together
with
a
solution
for
each
instance
Instances
are
questions
that
we
can
ask
and
solutions
are
desired
answers
to
these
questions
Theoretical
computer
science
seeks
to
understand
which
computational
problems
can
be
solved
by
using
a
computer
computability
theory
and
how
efficiently
computational
complexity
theory
Traditionally
it
is
said
that
a
problem
can
be
solved
by
using
a
computer
if
we
can
design
an
algorithm
that
produces
a
correct
solution
for
any
given
instance
Such
an
algorithm
can
be
implemented
as
a
computer
program
that
runs
on
a
generalpurpose
computer
the
program
reads
a
problem
instance
from
input
performs
some
computation
and
produces
the
solution
as
output
Formalisms
such
as
random
access
machines
or
universal
Turing
machines
can
be
used
as
abstract
models
of
a
sequential
generalpurpose
computer
executing
such
an
algorithm
The
field
of
concurrent
and
distributed
computing
studies
similar
questions
in
the
case
of
either
multiple
computers
or
a
computer
that
executes
a
network
of
interacting
processes
which
computational
problems
can
be
solved
in
such
a
network
and
how
efficiently?
However
it
is
not
at
all
obvious
what
is
meant
by
solving
a
problem
in
the
case
of
a
concurrent
or
distributed
system
for
example
what
is
the
task
of
the
algorithm
designer
and
what
is
the
concurrent
or
distributed
equivalent
of
a
sequential
generalpurpose
computer?
The
discussion
below
focuses
on
the
case
of
multiple
computers
although
many
of
the
issues
are
the
same
for
concurrent
processes
running
on
a
single
computer
Three
viewpoints
are
commonly
used
Parallel
algorithms
in
sharedmemory
model
All
processors
have
access
to
a
shared
memory
The
algorithm
designer
chooses
the
program
executed
by
each
processor
One
theoretical
model
is
the
parallel
random
access
machines
PRAM
that
are
used
However
the
classical
PRAM
model
assumes
synchronous
access
to
the
shared
memory
Sharedmemory
programs
can
be
extended
to
distributed
systems
if
the
underlying
operating
system
encapsulates
the
communication
between
nodes
and
virtually
unifies
the
memory
across
all
individual
systems
A
model
that
is
closer
to
the
behavior
of
realworld
multiprocessor
machines
and
takes
into
account
the
use
of
machine
instructions
such
as
Compareandswap
CAS
is
that
of
asynchronous
shared
memory
There
is
a
wide
body
of
work
on
this
model
a
summary
of
which
can
be
found
in
the
literature
Parallel
algorithms
in
messagepassing
model
The
algorithm
designer
chooses
the
structure
of
the
network
as
well
as
the
program
executed
by
each
computer
Models
such
as
Boolean
circuits
and
sorting
networks
are
used
A
Boolean
circuit
can
be
seen
as
a
computer
network
each
gate
is
a
computer
that
runs
an
extremely
simple
computer
program
Similarly
a
sorting
network
can
be
seen
as
a
computer
network
each
comparator
is
a
computer
Distributed
algorithms
in
messagepassing
model
The
algorithm
designer
only
chooses
the
computer
program
All
computers
run
the
same
program
The
system
must
work
correctly
regardless
of
the
structure
of
the
network
A
commonly
used
model
is
a
graph
with
one
finitestate
machine
per
node
In
the
case
of
distributed
algorithms
computational
problems
are
typically
related
to
graphs
Often
the
graph
that
describes
the
structure
of
the
computer
network
is
the
problem
instance
This
is
illustrated
in
the
following
example
An
example
Consider
the
computational
problem
of
finding
a
coloring
of
a
given
graph
G
Different
fields
might
take
the
following
approaches
Centralized
algorithms
The
graph
G
is
encoded
as
a
string
and
the
string
is
given
as
input
to
a
computer
The
computer
program
finds
a
coloring
of
the
graph
encodes
the
coloring
as
a
string
and
outputs
the
result
Parallel
algorithms
Again
the
graph
G
is
encoded
as
a
string
However
multiple
computers
can
access
the
same
string
in
parallel
Each
computer
might
focus
on
one
part
of
the
graph
and
produce
a
coloring
for
that
part
The
main
focus
is
on
highperformance
computation
that
exploits
the
processing
power
of
multiple
computers
in
parallel
Distributed
algorithms
The
graph
G
is
the
structure
of
the
computer
network
There
is
one
computer
for
each
node
of
G
and
one
communication
link
for
each
edge
of
G
Initially
each
computer
only
knows
about
its
immediate
neighbors
in
the
graph
G
the
computers
must
exchange
messages
with
each
other
to
discover
more
about
the
structure
of
G
Each
computer
must
produce
its
own
color
as
output
The
main
focus
is
on
coordinating
the
operation
of
an
arbitrary
distributed
system
While
the
field
of
parallel
algorithms
has
a
different
focus
than
the
field
of
distributed
algorithms
there
is
a
lot
of
interaction
between
the
two
fields
For
example
the
Cole–Vishkin
algorithm
for
graph
coloring
was
originally
presented
as
a
parallel
algorithm
but
the
same
technique
can
also
be
used
directly
as
a
distributed
algorithm
Moreover
a
parallel
algorithm
can
be
implemented
either
in
a
parallel
system
using
shared
memory
or
in
a
distributed
system
using
message
passing
The
traditional
boundary
between
parallel
and
distributed
algorithms
choose
a
suitable
network
vs
run
in
any
given
network
does
not
lie
in
the
same
place
as
the
boundary
between
parallel
and
distributed
systems
shared
memory
vs
message
passing
Complexity
measures
In
parallel
algorithms
yet
another
resource
in
addition
to
time
and
space
is
the
number
of
computers
Indeed
often
there
is
a
tradeoff
between
the
running
time
and
the
number
of
computers
the
problem
can
be
solved
faster
if
there
are
more
computers
running
in
parallel
see
speedup
If
a
decision
problem
can
be
solved
in
polylogarithmic
time
by
using
a
polynomial
number
of
processors
then
the
problem
is
said
to
be
in
the
class
NC
The
class
NC
can
be
defined
equally
well
by
using
the
PRAM
formalism
or
Boolean
circuits—PRAM
machines
can
simulate
Boolean
circuits
efficiently
and
vice
versa
In
the
analysis
of
distributed
algorithms
more
attention
is
usually
paid
on
communication
operations
than
computational
steps
Perhaps
the
simplest
model
of
distributed
computing
is
a
synchronous
system
where
all
nodes
operate
in
a
lockstep
fashion
This
model
is
commonly
known
as
the
LOCAL
model
During
each
communication
round
all
nodes
in
parallel
1
receive
the
latest
messages
from
their
neighbours
2
perform
arbitrary
local
computation
and
3
send
new
messages
to
their
neighbors
In
such
systems
a
central
complexity
measure
is
the
number
of
synchronous
communication
rounds
required
to
complete
the
task
This
complexity
measure
is
closely
related
to
the
diameter
of
the
network
Let
D
be
the
diameter
of
the
network
On
the
one
hand
any
computable
problem
can
be
solved
trivially
in
a
synchronous
distributed
system
in
approximately
2D
communication
rounds
simply
gather
all
information
in
one
location
D
rounds
solve
the
problem
and
inform
each
node
about
the
solution
D
rounds
On
the
other
hand
if
the
running
time
of
the
algorithm
is
much
smaller
than
D
communication
rounds
then
the
nodes
in
the
network
must
produce
their
output
without
having
the
possibility
to
obtain
information
about
distant
parts
of
the
network
In
other
words
the
nodes
must
make
globally
consistent
decisions
based
on
information
that
is
available
in
their
local
Dneighbourhood
Many
distributed
algorithms
are
known
with
the
running
time
much
smaller
than
D
rounds
and
understanding
which
problems
can
be
solved
by
such
algorithms
is
one
of
the
central
research
questions
of
the
field
Typically
an
algorithm
which
solves
a
problem
in
polylogarithmic
time
in
the
network
size
is
considered
efficient
in
this
model
Another
commonly
used
measure
is
the
total
number
of
bits
transmitted
in
the
network
cf
communication
complexity
The
features
of
this
concept
are
typically
captured
with
the
CONGESTB
model
which
similarly
defined
as
the
LOCAL
model
but
where
single
messages
can
only
contain
B
bits
Other
problems
Traditional
computational
problems
take
the
perspective
that
we
ask
a
question
a
computer
or
a
distributed
system
processes
the
question
for
a
while
and
then
produces
an
answer
and
stops
However
there
are
also
problems
where
we
do
not
want
the
system
to
ever
stop
Examples
of
such
problems
include
the
dining
philosophers
problem
and
other
similar
mutual
exclusion
problems
In
these
problems
the
distributed
system
is
supposed
to
continuously
coordinate
the
use
of
shared
resources
so
that
no
conflicts
or
deadlocks
occur
There
are
also
fundamental
challenges
that
are
unique
to
distributed
computing
The
first
example
is
challenges
that
are
related
to
faulttolerance
Examples
of
related
problems
include
consensus
problems
Byzantine
fault
tolerance
and
selfstabilisation
A
lot
of
research
is
also
focused
on
understanding
the
asynchronous
nature
of
distributed
systems
Synchronizers
can
be
used
to
run
synchronous
algorithms
in
asynchronous
systems
Logical
clocks
provide
a
causal
happenedbefore
ordering
of
events
Clock
synchronization
algorithms
provide
globally
consistent
physical
time
stamps
Election
Coordinator
election
or
leader
election
is
the
process
of
designating
a
single
process
as
the
organizer
of
some
task
distributed
among
several
computers
nodes
Before
the
task
is
begun
all
network
nodes
are
either
unaware
which
node
will
serve
as
the
coordinator
or
leader
of
the
task
or
unable
to
communicate
with
the
current
coordinator
After
a
coordinator
election
algorithm
has
been
run
however
each
node
throughout
the
network
recognizes
a
particular
unique
node
as
the
task
coordinator
The
network
nodes
communicate
among
themselves
in
order
to
decide
which
of
them
will
get
into
the
coordinator
state
For
that
they
need
some
method
in
order
to
break
the
symmetry
among
them
For
example
if
each
node
has
unique
and
comparable
identities
then
the
nodes
can
compare
their
identities
and
decide
that
the
node
with
the
highest
identity
is
the
coordinator
The
definition
of
this
problem
is
often
attributed
to
LeLann
who
formalized
it
as
a
method
to
create
a
new
token
in
a
token
ring
network
in
which
the
token
has
been
lost
Coordinator
election
algorithms
are
designed
to
be
economical
in
terms
of
total
bytes
transmitted
and
time
The
algorithm
suggested
by
Gallager
Humblet
and
Spira

for
general
undirected
graphs
has
had
a
strong
impact
on
the
design
of
distributed
algorithms
in
general
and
won
the
Dijkstra
Prize
for
an
influential
paper
in
distributed
computing
Many
other
algorithms
were
suggested
for
different
kind
of
network
graphs
such
as
undirected
rings
unidirectional
rings
complete
graphs
grids
directed
Euler
graphs
and
others
A
general
method
that
decouples
the
issue
of
the
graph
family
from
the
design
of
the
coordinator
election
algorithm
was
suggested
by
Korach
Kutten
and
Moran
In
order
to
perform
coordination
distributed
systems
employ
the
concept
of
coordinators
The
coordinator
election
problem
is
to
choose
a
process
from
among
a
group
of
processes
on
different
processors
in
a
distributed
system
to
act
as
the
central
coordinator
Several
central
coordinator
election
algorithms
exist
Properties
of
distributed
systems
So
far
the
focus
has
been
on
designing
a
distributed
system
that
solves
a
given
problem
A
complementary
research
problem
is
studying
the
properties
of
a
given
distributed
system
The
halting
problem
is
an
analogous
example
from
the
field
of
centralised
computation
we
are
given
a
computer
program
and
the
task
is
to
decide
whether
it
halts
or
runs
forever
The
halting
problem
is
undecidable
in
the
general
case
and
naturally
understanding
the
behaviour
of
a
computer
network
is
at
least
as
hard
as
understanding
the
behaviour
of
one
computer
However
there
are
many
interesting
special
cases
that
are
decidable
In
particular
it
is
possible
to
reason
about
the
behaviour
of
a
network
of
finitestate
machines
One
example
is
telling
whether
a
given
network
of
interacting
asynchronous
and
nondeterministic
finitestate
machines
can
reach
a
deadlock
This
problem
is
PSPACEcomplete
ie
it
is
decidable
but
it
is
not
likely
that
there
is
an
efficient
centralised
parallel
or
distributed
algorithm
that
solves
the
problem
in
the
case
of
large
networks
See
also
AppScale
BOINC
Code
mobility
Decentralized
computing
Distributed
algorithm
Distributed
algorithmic
mechanism
design
Distributed
cache
Distributed
operating
system
Edsger
W
Dijkstra
Prize
in
Distributed
Computing
Fog
computing
Folding@home
Grid
computing
Inferno
Jungle
computing
Layered
queueing
network
Library
Oriented
Architecture
LOA
List
of
distributed
computing
conferences
List
of
distributed
computing
projects
List
of
important
publications
in
concurrent
parallel
and
distributed
computing
Model
checking
Parallel
distributed
processing
Parallel
programming
model
Plan
9
from
Bell
Labs
Shared
nothing
architecture
Notes
^
a
b
Coulouris
George
Jean
Dollimore
Tim
Kindberg
Gordon
Blair
2011
Distributed
Systems
Concepts
and
Design
5th
Edition
Boston
AddisonWesley
ISBN
0132143011
^
Andrews
2000
Dolev
2000
Ghosh
2007
p
10
^
Magnoni
L
2015
Modern
Messaging
for
Distributed
Sytems
Journal
of
Physics
Conference
Series
608
1
012038
doi101088/17426596/608/1/012038
ISSN
17426596
^
Godfrey
2002
^
a
b
Andrews
2000
p
291–292
Dolev
2000
p
5
^
Lynch
1996
p
1
^
a
b
Ghosh
2007
p
10
^
Andrews
2000
pp
8–9
291
Dolev
2000
p
5
Ghosh
2007
p
3
Lynch
1996
p
xix
1
Peleg
2000
p
xv
^
Andrews
2000
p
291
Ghosh
2007
p
3
Peleg
2000
p
4
^
Ghosh
2007
p
3–4
Peleg
2000
p
1
^
Ghosh
2007
p
4
Peleg
2000
p
2
^
Ghosh
2007
p
4
8
Lynch
1996
p
2–3
Peleg
2000
p
4
^
Lynch
1996
p
2
Peleg
2000
p
1
^
Ghosh
2007
p
7
Lynch
1996
p
xix
2
Peleg
2000
p
4
^
Ghosh
2007
p
10
Keidar
2008
^
Lynch
1996
p
xix
1–2
Peleg
2000
p
1
^
Peleg
2000
p
1
^
Papadimitriou
1994
Chapter
15
Keidar
2008
^
See
references
in
Introduction
^
Bentaleb
A
Yifan
L
Xin
J
et
al
2016
Parallel
and
Distributed
Algorithms
PDF
National
University
of
Singapore
Retrieved
20
July
2018CS1
maint
Explicit
use
of
et
al
link
CS1
maint
Multiple
names
authors
list
link
^
Andrews
2000
p
348
^
Andrews
2000
p
32
^
Peter
2004
The
history
of
email
^
Banks
M
2012
On
the
Way
to
the
Web
The
Secret
History
of
the
Internet
and
its
Founders
Apress
pp
44–5
ISBN
9781430250746
^
Tel
G
2000
Introduction
to
Distributed
Algorithms
Cambridge
University
Press
pp
35–36
ISBN
9780521794831
^
Ohlídal
M
Jaroš
J
Schwarz
J
et
al
2006
Evolutionary
Design
of
OAB
and
AAB
Communication
Schedules
for
Interconnection
Networks
In
Rothlauf
F
Branke
J
Cagnoni
S
et
al
Applications
of
Evolutionary
Computing
Springer
Science
&
Business
Media
pp
267–78
ISBN
9783540332374CS1
maint
Explicit
use
of
et
al
link
CS1
maint
Multiple
names
authors
list
link
^
Real
Time
And
Distributed
Computing
Systems
PDF
ISSN
22780661
Retrieved
20170109
^
Vigna
P
Casey
MJ
The
Age
of
Cryptocurrency
How
Bitcoin
and
the
Blockchain
Are
Challenging
the
Global
Economic
Order
St
Martin's
Press
January
27
2015
ISBN
9781250065636
^
Hieu
Vu
Quang
2010
Peertopeer
computing

principles
and
applications
Lupu
Mihai
Ooi
Beng
Chin
1961
Heidelberg
Springer
p
16
ISBN
9783642035135
OCLC
663093862
^
Lind
P
Alm
M
2006
A
databasecentric
virtual
chemistry
system
J
Chem
Inf
Model
46
3
1034–9
doi101021/ci050360b
PMID
16711722
^
Elmasri
&
Navathe
2000
Section
2412
^
Andrews
2000
p
10–11
Ghosh
2007
p
4–6
Lynch
1996
p
xix
1
Peleg
2000
p
xv
Elmasri
&
Navathe
2000
Section
24
^
Toomarian
NB
Barhen
J
Gulati
S
1992
Neural
Networks
for
RealTime
Robotic
Applications
In
Fijany
A
Bejczy
A
Parallel
Computation
Systems
For
Robotics
Algorithms
And
Architectures
World
Scientific
p
214
ISBN
9789814506175CS1
maint
Multiple
names
authors
list
link
^
Savage
JE
1998
Models
of
Computation
Exploring
the
Power
of
Computing
Addison
Wesley
p
209
ISBN
9780201895391
^
Cormen
Leiserson
&
Rivest
1990
Section
30
^
Herlihy
&
Shavit
2008
Chapters
26
^
Lynch
1996
^
Cormen
Leiserson
&
Rivest
1990
Sections
28
and
29
^
Cole
&
Vishkin
1986
Cormen
Leiserson
&
Rivest
1990
Section
305
^
Andrews
2000
p
ix
^
Arora
&
Barak
2009
Section
67
Papadimitriou
1994
Section
153
^
Papadimitriou
1994
Section
152
^
Lynch
1996
p
17–23
^
Peleg
2000
Sections
23
and
7
Linial
1992
Naor
&
Stockmeyer
1995
^
Schneider
J
Wattenhofer
R
2011
Trading
Bit
Message
and
Time
Complexity
of
Distributed
Algorithms
In
Peleg
D
Distributed
Computing
Springer
Science
&
Business
Media
pp
51–65
ISBN
9783642240997CS1
maint
Multiple
names
authors
list
link
^
Lynch
1996
Sections
5–7
Ghosh
2007
Chapter
13
^
Lynch
1996
p
99–102
Ghosh
2007
p
192–193
^
Dolev
2000
Ghosh
2007
Chapter
17
^
Lynch
1996
Section
16
Peleg
2000
Section
6
^
Lynch
1996
Section
18
Ghosh
2007
Sections
62–63
^
Ghosh
2007
Section
64
^
a
b
Haloi
S
2015
Apache
ZooKeeper
Essentials
Packt
Publishing
Ltd
pp
100–101
ISBN
9781784398323
^
LeLann
G
1977
Distributed
systems

toward
a
formal
approach
Information
Processing
77
155·160
–
via
Elsevier
^
R
G
Gallager
P
A
Humblet
and
P
M
Spira
January
1983
A
Distributed
Algorithm
for
MinimumWeight
Spanning
Trees
PDF
ACM
Transactions
on
Programming
Languages
and
Systems
5
1
66–77
doi101145/357195357200CS1
maint
Multiple
names
authors
list
link
^
Korach
Ephraim
Kutten
Shay
Moran
Shlomo
1990
A
Modular
Technique
for
the
Design
of
Efficient
Distributed
Leader
Finding
Algorithms
PDF
ACM
Transactions
on
Programming
Languages
and
Systems
12
1
84–101
doi101145/7760677610
^
Hamilton
Howard
Distributed
Algorithms
Retrieved
20130303
^
Major
unsolved
problems
in
distributed
systems?
cstheorystackexchangecom
Retrieved
16
March
2018
^
How
big
data
and
distributed
systems
solve
traditional
scalability
problems
theserversidecom
Retrieved
16
March
2018
^
Svozil
K
2011
Indeterminism
and
Randomness
Through
Physics
In
Hector
Z
Randomness
Through
Computation
Some
Answers
More
Questions
World
Scientific
pp
112–3
ISBN
9789814462631
^
Papadimitriou
1994
Section
193
References
Books
Andrews
Gregory
R
2000
Foundations
of
Multithreaded
Parallel
and
Distributed
Programming
Addison–Wesley
ISBN
0201357526

Arora
Sanjeev
Barak
Boaz
2009
Computational
Complexity
–
A
Modern
Approach
Cambridge
ISBN
9780521424264

Cormen
Thomas
H
Leiserson
Charles
E
Rivest
Ronald
L
1990
Introduction
to
Algorithms
1st
ed
MIT
Press
ISBN
0262031418

Dolev
Shlomi
2000
SelfStabilization
MIT
Press
ISBN
0262041782

Elmasri
Ramez
Navathe
Shamkant
B
2000
Fundamentals
of
Database
Systems
3rd
ed
Addison–Wesley
ISBN
0201542633

Ghosh
Sukumar
2007
Distributed
Systems
–
An
Algorithmic
Approach
Chapman
&
Hall/CRC
ISBN
9781584885641

Lynch
Nancy
A
1996
Distributed
Algorithms
Morgan
Kaufmann
ISBN
1558603484

Herlihy
Maurice
P
Shavit
Nir
N
2008
The
Art
of
Multiprocessor
Programming
Morgan
Kaufmann
ISBN
0123705916

Papadimitriou
Christos
H
1994
Computational
Complexity
Addison–Wesley
ISBN
0201530821

Peleg
David
2000
Distributed
Computing
A
LocalitySensitive
Approach
SIAM
ISBN
0898714648

Articles
Cole
Richard
Vishkin
Uzi
1986
Deterministic
coin
tossing
with
applications
to
optimal
parallel
list
ranking
Information
and
Control
70
1
32–53
doi101016/S0019995886800237

Keidar
Idit
2008
Distributed
computing
column
32
–
The
year
in
review
ACM
SIGACT
News
39
4
53–54
doi101145/14663901466402

Linial
Nathan
1992
Locality
in
distributed
graph
algorithms
SIAM
Journal
on
Computing
21
1
193–201
CiteSeerX
10114716378
doi101137/0221015

Naor
Moni
Stockmeyer
Larry
1995
What
can
be
computed
locally?
PDF
SIAM
Journal
on
Computing
24
6
1259–1277
doi101137/S0097539793254571

Web
sites
Godfrey
Bill
2002
A
primer
on
distributed
computing
Peter
Ian
2004
Ian
Peter's
History
of
the
Internet
Retrieved
20090804
Further
reading
Books
Attiya
Hagit
and
Jennifer
Welch
2004
Distributed
Computing
Fundamentals
Simulations
and
Advanced
Topics
WileyInterscience
ISBN
0471453242
Christian
Cachin
Rachid
Guerraoui
Luís
Rodrigues
2011
Introduction
to
Reliable
and
Secure
Distributed
Programming
2
ed
Springer
ISBN
9783642152597
Coulouris
George
et
al
2011
Distributed
Systems
Concepts
and
Design
5th
Edition
AddisonWesley
ISBN
0132143011
Faber
Jim
1998
Java
Distributed
Computing
O'Reilly

Java
Distributed
Computing
by
Jim
Faber
1998
Garg
Vijay
K
2002
Elements
of
Distributed
Computing
WileyIEEE
Press
ISBN
0471036005
Tel
Gerard
1994
Introduction
to
Distributed
Algorithms
Cambridge
University
Press
Chandy
Mani
et
al
Parallel
Program
Design
Articles
Keidar
Idit
Rajsbaum
Sergio
eds
2000–2009
Distributed
computing
column
ACM
SIGACT
News

Birrell
A
D
Levin
R
Schroeder
M
D
Needham
R
M
April
1982
Grapevine
An
exercise
in
distributed
computing
PDF
Communications
of
the
ACM
25
4
260–274
doi101145/358468358487
Conference
Papers
C
Rodríguez
M
Villagra
and
B
Barán
Asynchronous
team
algorithms
for
Boolean
Satisfiability
Bionetics2007
pp
66–69
2007
External
links
Wikimedia
Commons
has
media
related
to
Distributed
computing
Distributed
computing
at
Curlie
Distributed
computing
journals
at
Curlie
v
t
e
Parallel
computing
General
Distributed
computing
Parallel
computing
Massively
parallel
Cloud
computing
Highperformance
computing
Multiprocessing
Manycore
processor
GPGPU
Computer
network
Systolic
array
Levels
Bit
Instruction
Thread
Task
Data
Memory
Loop
Pipeline
Multithreading
Temporal
Simultaneous
SMT
Speculative
SpMT
Preemptive
Cooperative
Clustered
MultiThread
CMT
Hardware
scout
Theory
PRAM
model
Analysis
of
parallel
algorithms
Amdahl's
law
Gustafson's
law
Cost
efficiency
Karp–Flatt
metric
Slowdown
Speedup
Elements
Process
Thread
Fiber
Instruction
window
Array
data
structure
Coordination
Multiprocessing
Memory
coherency
Cache
coherency
Cache
invalidation
Barrier
Synchronization
Application
checkpointing
Programming
Stream
processing
Dataflow
programming
Models
Implicit
parallelism
Explicit
parallelism
Concurrency
Nonblocking
algorithm
Hardware
Flynn's
taxonomy
SISD
SIMD
SIMT
MISD
MIMD
Dataflow
architecture
Pipelined
processor
Superscalar
processor
Vector
processor
Multiprocessor
symmetric
asymmetric
Memory
shared
distributed
distributed
shared
UMA
NUMA
COMA
Massively
parallel
computer
Computer
cluster
Grid
computer
Hardware
acceleration
APIs
Ateji
PX
BoostThread
Chapel
Charm++
Cilk
Coarray
Fortran
CUDA
Dryad
C++
AMP
Global
Arrays
MPI
OpenMP
OpenCL
OpenHMPP
OpenACC
TPL
PLINQ
PVM
POSIX
Threads
RaftLib
UPC
TBB
ZPL
Problems
Deadlock
Livelock
Deterministic
algorithm
Embarrassingly
parallel
Parallel
slowdown
Race
condition
Software
lockout
Scalability
Starvation
Category
parallel
computing
Media
related
to
Parallel
computing
at
Wikimedia
Commons
v
t
e
Edsger
Dijkstra
Notable
works
A
Primer
of
ALGOL
60
Programming
book
Structured
Programming
book
A
Discipline
of
Programming
book
A
Method
of
Programming
book
Predicate
Calculus
and
Program
Semantics
book
Selected
Writings
on
Computing
A
Personal
Perspective
book
Selected
papers
EWD
manuscripts
A
Note
on
Two
Problems
in
Connexion
with
Graphs
Cooperating
Sequential
Processes
Solution
of
a
Problem
in
Concurrent
Programming
Control
The
Structure
of
the
'THE'Multiprogramming
System
Go
To
Statement
Considered
Harmful
Notes
on
Structured
Programming
The
Humble
Programmer
Programming
Considered
as
a
Human
Activity
How
Do
We
Tell
Truths
That
Might
Hurt?
On
the
Role
of
Scientific
Thought
Selfstabilizing
Systems
in
Spite
of
Distributed
Control
On
the
Cruelty
of
Really
Teaching
Computer
Science
Main
research
areas
Theoretical
computing
science
Software
engineering
Systems
science
Algorithm
design
Concurrent
computing
Distributed
computing
Formal
methods
Programming
methodology
Programming
language
research
Program
design
and
development
Software
architecture
Philosophy
of
computer
programming
and
computing
science
Scientific
contributions
Concepts
and
methods
ALGOL
60
implementation
Call
stack
Concurrency
Concurrent
programming
Cooperating
sequential
processes
Critical
section
Deadly
embrace
deadlock
Dining
philosophers
problem
Dutch
national
flag
problem
Faulttolerant
system
Gotoless
programming
Guarded
Command
Language
Layered
structure
in
software
architecture
Levels
of
abstraction
Multithreaded
programming
Mutual
exclusion
mutex
Producer–consumer
problem
bounded
buffer
problem
Program
families
Predicate
transformer
semantics
Process
synchronization
Selfstabilizing
distributed
system
Semaphore
programming
Separation
of
concerns
Sleeping
barber
problem
Software
crisis
Structured
analysis
Structured
programming
THE
multiprogramming
system
Unbounded
nondeterminism
Weakest
precondition
calculus
Algorithms
Banker's
algorithm
Dijkstra's
algorithm
DJP
algorithm
Prim's
algorithm
DijkstraScholten
algorithm
Dekker's
algorithm
generalization
Smoothsort
Shuntingyard
algorithm
Tricolor
marking
algorithm
Concurrent
algorithms
Distributed
algorithms
Deadlock
prevention
algorithms
Mutual
exclusion
algorithms
Selfstabilizing
algorithms
Related
people
Shlomi
Dolev
Per
Brinch
Hansen
Tony
Hoare
OleJohan
Dahl
Leslie
Lamport
David
Parnas
Adriaan
van
Wijngaarden
Niklaus
Wirth
Other
topics
Dijkstra
Prize
Edsger
W
Dijkstra
Prize
in
Distributed
Computing
Centrum
Wiskunde
&
Informatica
EW
Dijkstra
Archive
University
of
Texas
at
Austin
List
of
pioneers
in
computer
science
List
of
important
publications
in
computer
science
List
of
important
publications
in
theoretical
computer
science
List
of
important
publications
in
concurrent
parallel
and
distributed
computing
International
S